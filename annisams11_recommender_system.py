# -*- coding: utf-8 -*-
"""annisams11-recommender-system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oL3JriQ_b4ONCebnvdqcPc07wgTKaW2i

# **Skincare Product Recommender System**

<p align="right">by Annisa Mufidatun Sholihah</p>

## **Import library dan dataset**

**Import library that needed**
"""

# import library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import mean_squared_error, mean_absolute_error
import tensorflow
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import RootMeanSquaredError
import random

"""**Import dataset from kaggle**

[link to dataset](https://www.kaggle.com/datasets/nadyinky/sephora-products-and-skincare-reviews)
"""

#import kaggle dataset
!pip install kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d nadyinky/sephora-products-and-skincare-reviews
!unzip sephora-products-and-skincare-reviews.zip

"""## **Create dataset**

create data frame for **product** dataset
"""

product = pd.read_csv('/content/product_info.csv')
product

"""Merged all review dataset into one DataFrame **reviews**"""

review_0 = pd.read_csv('/content/reviews_0-250.csv')
review_1 = pd.read_csv('/content/reviews_250-500.csv')
review_2 = pd.read_csv('/content/reviews_500-750.csv')
review_3 = pd.read_csv('/content/reviews_750-1250.csv')
review_4 = pd.read_csv('/content/reviews_1250-end.csv')


# Gabungkan semua file review menjadi satu DataFrame
reviews = pd.concat([review_0, review_1, review_2, review_3, review_4], ignore_index=True)
reviews

"""## **Exploratory Data Analysis**

Displays information about the 'product' and 'reviews' DataFrames, including their data types and non-null counts. It then prints out the column names for both DataFrames, providing an overview of the structure and content of each dataset.
"""

product.shape

reviews.shape

product.info()

reviews.info()

# List columns in the 'product' DataFrame
print("Columns in 'product' DataFrame:")
print(product.columns.tolist())

# List columns in the 'reviews' DataFrame
print("\nColumns in 'reviews' DataFrame:")
print(reviews.columns.tolist())

"""View summary statistics for the numeric columns in both the 'product' and 'reviews' DataFrames."""

product.describe()

reviews.describe()

"""Check null values in both the 'product' and 'reviews' DataFrames."""

product.isnull().sum()

reviews.isnull().sum()

"""Check if there is an duplicate row in the 'product' and 'reviews' DataFrames."""

product.duplicated().sum()

reviews.duplicated().sum()

"""Examines categorical features in both 'product' and 'reviews' DataFrames. It loops through specified categorical columns, printing the unique values for each."""

# categorical features
categorical_features = ['brand_name', 'size', 'variation_value', 'ingredients',
                        'highlights', 'primary_category', 'secondary_category', 'tertiary_category']

# Loop melalui setiap fitur kategorikal
for feature in categorical_features:
    unique_values = product[feature].unique()
    print(f"Unique values in '{feature}':")
    print(product[feature].unique())
    print()

# categorical features
categorical_features2 = ['skin_tone', 'eye_color', 'skin_type', 'hair_color', 'product_id', 'product_name', 'brand_name', ]

# Loop melalui setiap fitur kategorikal
for feature in categorical_features2:
    unique_values = reviews[feature].unique()
    print(f"Unique values in '{feature}':")
    print(reviews[feature].unique())
    print()

"""## **data preparation**

### **dataset product**

Choose only product that categorize as skincare
"""

product = product[product['primary_category'] == 'Skincare']
product

#new data shape
product.shape

product_category = product['secondary_category'].unique()
print("Total # of genre: ", len(product_category))
print("List of all genre availabel: ", product_category)

product_category = product['tertiary_category'].unique()
print("Total # of genre: ", len(product_category))
print("List of all genre availabel: ", product_category)

"""count products for each category."""

# Count rows for each category in 'tertiary_category'
tertiary_category_counts = product['tertiary_category'].value_counts()
print("Count of rows for each category in 'tertiary_category':")
print(tertiary_category_counts)

# Count null values in 'tertiary_category'
null_count = product['tertiary_category'].isnull().sum()
print("\nNumber of null values in 'tertiary_category':", null_count)

"""Fill null value in tertiary_category with the value in secondary_category."""

# Fill null values in 'tertiary_category' with values from 'secondary_category'
product['tertiary_category'] = product['tertiary_category'].fillna(product['secondary_category'])

# Count null values in 'tertiary_category' after filling
null_count_after_filling = product['tertiary_category'].isnull().sum()
print("\nNumber of null values in 'tertiary_category' after filling:", null_count_after_filling)

# Display the DataFrame to show the result
product

"""Examine the current category for tertiary_category"""

product_category = product['tertiary_category'].unique()
print("Total # of genre: ", len(product_category))
print("List of all genre availabel: ", product_category)

"""create new dataframe with needed features `'product_id'`, `'product_name'`, `'brand_name'`,	`'tertiary_category'`, `'rating'`

tertiary_category is choosen because there are more category.
"""

products = product[['product_id', 'product_name', 'brand_name',	'tertiary_category', 'rating']]
products

"""rename tertiary_category column"""

products.rename(columns={'tertiary_category': 'category'}, inplace=True)
products

num_unique_products = products['product_id'].nunique()
print("Number of unique products:", num_unique_products)

# Group the DataFrame by 'brand_name' and count the number of products for each brand
brand_counts = products['brand_name'].value_counts()

# Select the top 5 brands
top_5_brands = brand_counts.head()

# Create a bar plot for the top 5 brands
plt.figure(figsize=(10, 6))
top_5_brands.plot(kind='bar')
plt.title('Top 5 Brands by Number of Products')
plt.xlabel('Brand Name')
plt.ylabel('Number of Products')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Create a histogram for the 'rating' column
plt.figure(figsize=(8, 6))
plt.hist(products['rating'], bins=10, edgecolor='black')
plt.title('Distribution of Product Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()

"""### **dataset reviews**

check null value
"""

reviews.isnull().sum()

"""drop rows that have null value"""

reviews = reviews.dropna()
reviews

"""Create new dataframe with needed features `'author_id'`, `'rating'`, `'product_id'`"""

reviews = reviews[['author_id', 'rating', 'product_id']]
reviews

num_unique_product = reviews['product_id'].nunique()
print("Number of unique authors:", num_unique_product)

num_unique_authors = reviews['author_id'].nunique()
print("Number of unique authors:", num_unique_authors)

# prompt: create barchart for rating and sort from 1,2,3,4 and 5

# Group the DataFrame by 'rating' and count the number of reviews for each rating
rating_counts = reviews['rating'].value_counts().sort_index()

# Create a bar plot for the rating counts
plt.figure(figsize=(8, 6))
rating_counts.plot(kind='bar')
plt.title('Distribution of Ratings in Review dataset')
plt.xlabel('Rating')
plt.ylabel('Number of Reviews')
plt.xticks(rotation=0)
plt.show()

"""#### **Prepare data for colaborative filtering**

merge the product and reviews datasets and ensure that only the reviews for which there is a corresponding product_id in the product dataset are included
"""

data_2 = pd.merge(reviews, products, on='product_id', how='inner')
data_2

# Count unique values in 'author_id'
num_unique_authors = data_2['author_id'].nunique()
print("Number of unique authors:", num_unique_authors)

# Count unique values in 'product_id'
num_unique_products = data_2['product_id'].nunique()
print("Number of unique products:", num_unique_products)

product_counts = data_2['product_id'].value_counts()
print(product_counts)

"""**create a DataFrame with a total of 100,000 rows while maintaining a diverse set of unique product_id values.**"""

#Ensure all products are included
all_products = data_2['product_id'].unique()
one_of_each = data_2.groupby('product_id').first().reset_index()

#Calculate how many more rows we need
rows_to_sample = 100000 - len(all_products)

#Sample remaining rows
sampled_rows = data_2.sample(n=rows_to_sample, replace=True)

#Combine one of each product with sampled rows
result = pd.concat([one_of_each, sampled_rows], ignore_index=True)

#Shuffle the results
result = result.sample(frac=1).reset_index(drop=True)

# Verify the result
print(f"Total rows: {len(result)}")
print(f"Unique products: {result['product_id'].nunique()}")

# Count unique values in 'author_id'
num_unique_authors = result['author_id'].nunique()
print("Number of unique authors:", num_unique_authors)

data_2 = result
data_2

"""drop column that not needed"""

data_2 = data_2.drop(['product_name', 'brand_name', 'category', 'rating_y'], axis=1)
data_2.rename(columns={'rating_x': 'rating'}, inplace=True)
data_2

"""#### **encoding**"""

author_ids = data_2['author_id'].unique().tolist()
user2user_encoded = {x: i for i, x in enumerate(author_ids)}
user_encoded2user = {i: x for i, x in enumerate(author_ids)}
data_2['user'] = data_2['author_id'].map(user2user_encoded)

print('encoded angka ke userID: ', user2user_encoded)

product_ids = data_2['product_id'].unique().tolist()
product2product_encoded = {x: i for i, x in enumerate(product_ids)}
product_encoded2product = {i: x for i, x in enumerate(product_ids)}
data_2['product'] = data_2['product_id'].map(product2product_encoded)

print('encoded angka ke userID: ', product2product_encoded)

num_users = len(user_encoded2user)
print(num_users)

num_product = len(product_encoded2product)
print(num_product)

# change rating data type
data_2['rating'] = data_2['rating'].values.astype(np.float32)

# rating minimum value
min_rating = min(data_2['rating'])

# rating maximum value
max_rating = max(data_2['rating'])

print('Number of User: {}, Number of Product: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_product, min_rating, max_rating
))

# randomize dataset
data_2 = data_2.sample(frac=1, random_state=42)
data_2

"""**split dataset to train and validation**"""

# Create a variable x to match user and product data into one value
x = data_2[['user', 'product']].values

# Create a y variable to create a rating of the results
y = data_2['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Divide into 80% train data and 20% validation data
train_indices = int(0.8 * data_2.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""## **Modelling**

### **Content Based Filtering**

Done using the cosine similarity calculation of the skincare product category
"""

data_1 = products.copy()
data_1.head()

"""IThis code initializes a `TfidfVectorizer`, calculates TF-IDF scores for the `category` column in `data_1`, and creates a DataFrame displaying a sample of the TF-IDF matrix, with rows corresponding to product names and columns to feature names.

"""

# Initialize TfidfVectorizer
tfidf = TfidfVectorizer()

# Perform IDF calculations on cuisine data
tfidf.fit(data_1['category'])

# Mapping array from integer index features to name features
tfidf.get_feature_names_out()

tfidf_matrix = tfidf.fit_transform(data_1['category'])

# View the tfidf matrix size
tfidf_matrix.shape

tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf.get_feature_names_out(),
    index=data_1.product_name
).sample(22, axis=1).sample(10, axis=0)

"""**compute cosine similarity**"""

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

cosine_sim_df = pd.DataFrame(cosine_sim, index=data_1['product_name'], columns=data_1['product_name'])
print('Shape:', cosine_sim_df.shape)

# View the similarity matrix for each product
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""**Create product recommendation with cosine similarity**"""

def skincare_recommendations(product_name, similarity_data=cosine_sim_df, items=data_1[['product_name', 'brand_name', 'category']], k=5):
    """
    Provides product recommendations based on similarity.

    Parameters:
    ---
    product_name : str
        The name of the product for which recommendations are sought. This should be present in the index of the similarity DataFrame.
    similarity_data : pd.DataFrame
        A DataFrame containing similarity scores, symmetric, with products as both index and columns.
    items : pd.DataFrame
        A DataFrame containing product names and other features used to define similarity.
    k : int
        The number of recommendations to return.
    ---

    The function retrieves the top k most similar products based on the similarity scores provided in the similarity DataFrame.
    """

    # Convert the similarity scores for the given product into a numpy array and find the indices of the top k most similar products
    index = similarity_data.loc[:, product_name].to_numpy().argpartition(range(-1, -k, -1))

    # Retrieve the names of the top k most similar products
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Exclude the original product from the list of recommendations
    closest = closest.drop(product_name, errors='ignore')

    # Return the top k recommended products along with their features
    return pd.DataFrame(closest).merge(items).head(k)

#code to not truncate product name
pd.set_option('display.max_colwidth', None)

data_1[data_1.product_name.eq('Retinol Recovery Eye Cream')]
skincare_recommendations('Retinol Recovery Eye Cream')

"""**Evaluate Result**

Because all the result came out with same category, Eye Creams & Treatments, we can conclude that the result is relevant. Here's how to evaluate the result using **Recall at K**.
"""

# Define the prediction results
predicted_products = [
    'Pep-Start Eye Cream',
    'Eye Doctor Moisture Care For Skin Around Eyes',
    'Ginzing Vitamin C Eye Cream to Brighten and Depuff',
    'Brightening Eye Cream Mineral SPF 15 with Peptides	',
    'Green Tea Hyaluronic Acid Hydrating Eye Cream'
]

# Define the relevant products
relevant_products = [
    'Pep-Start Eye Cream',
    'Eye Doctor Moisture Care For Skin Around Eyes',
    'Ginzing Vitamin C Eye Cream to Brighten and Depuff',
    'Brightening Eye Cream Mineral SPF 15 with Peptides	',
    'Green Tea Hyaluronic Acid Hydrating Eye Cream'
]

# Define K (number of top recommendations)
k = 5

# Function to compute recall at K
def recall_at_k(predicted, relevant, k):
    predicted_k = predicted[:k]  # Top K predicted items
    relevant_count = len(set(predicted_k) & set(relevant))  # Count of relevant items in top K
    return relevant_count / len(relevant) if relevant else 0

# Calculate recall at K
recall_score = recall_at_k(predicted_products, relevant_products, k)
print(f"Recall at {k}: {recall_score * 100:.2f}%")

"""### **Collaborative filtering**

Create neural network model for recommendation systems using embeddings to represent users and products. Calculates the predicted rating by combining user and product embeddings with biases, and applies a sigmoid activation function to the result.
"""

class RecommenderNet(keras.Model):
    def __init__(self, num_users, num_product, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_product = num_product
        self.embedding_size = embedding_size
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)
        self.product_embedding = layers.Embedding(
            num_product,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.product_bias = layers.Embedding(num_product, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        product_vector = self.product_embedding(inputs[:, 1])
        product_bias = self.product_bias(inputs[:, 1])

        dot_user_product = tensorflow.tensordot(user_vector, product_vector, 2)

        x = dot_user_product + user_bias + product_bias

        return tensorflow.nn.sigmoid(x)

model = RecommenderNet(num_users, num_product, 50)

model.compile(
    loss=BinaryCrossentropy(),
    optimizer=Adam(learning_rate=0.001),
    metrics=[RootMeanSquaredError()]
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 128,
    epochs = 50,
    validation_data = (x_val, y_val)
)

"""**plot metrics model**"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""**Create product recommendation with created model**"""

import random

def get_recommendations(model, user_id, product_df, num_recommendations=10):
    # Get a list of all products
    all_products = list(product2product_encoded.values())

    # Get products the user has already rated
    user_products = data_2[data_2['user'] == user2user_encoded[user_id]]['product'].tolist()

    # Find products the user hasn't rated
    products_to_predict = list(set(all_products) - set(user_products))

    # Create input data for predictions
    user_array = np.array([[user2user_encoded[user_id]] * len(products_to_predict)])
    product_array = np.array([products_to_predict])
    user_product_array = np.transpose(np.concatenate([user_array, product_array]))

    # Make predictions
    predictions = model.predict(user_product_array).flatten()

    # Get top N recommendations
    top_indices = predictions.argsort()[-num_recommendations:][::-1]
    recommended_product_ids = [product_encoded2product[products_to_predict[x]] for x in top_indices]

    # Get additional product information
    recommended_products = product_df[product_df['product_id'].isin(recommended_product_ids)].copy()
    recommended_products['predicted_rating'] = predictions[top_indices]
    recommended_products = recommended_products.sort_values('predicted_rating', ascending=False)

    return recommended_products

#load product data
product_df = products.copy()

# Select a random user
random_user = random.choice(list(user2user_encoded.keys()))

# Get recommendations
recommendations = get_recommendations(model, random_user, product_df)

# Print recommendations
print(f"Top 10 recommendations for user {random_user}:")
for i, (_, row) in enumerate(recommendations.iterrows(), 1):
    print(f"{i}. Product: {row['product_name']}")
    print(f"   Brand: {row['brand_name']}")
    print(f"   Category: {row['category']}")
    print(f"   Actual Rating: {row['rating']}")
    print()